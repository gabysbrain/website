---
title: Sensitivity analysis in practice, chapter 2 and 8 notes
---

## SA workshop

* how well do you have to know the model?
* data science: given outputs, don't know the model
* simulation science: given the model, don't know the outputs

## Data science vs modeling

* __ML__: start with data, what is a model?
    * inputs and outputs not crisply defined
* __Sims__: start with model, what are the outputs?
* this is similar to SciVis vs InfoVis

__Saltelli definition__: the study of how the output of a model can be
  proportioned to different sources of uncertainty in the input

* This doesn't fit really w/ data science
* Don't know all inputs that generated the output
* simulation science: build a model and then examine it to divine physical
  science principles from that model
* simulation people: a good model can be reduced into its component parts
  and studied individually
* machine learning: don't want to study the component parts, model can't
  be reduced
* model vs map: model has semantic meaning of what the inputs, outputs, and
  mappings mean and map is much more abstract


## How to organize data

### Mental models

* some people think about data as a mapping of inputs to outputs, some see
  it as just a table of numbers, etc
* adding outputs to objects inherently makes them part of a
  process semantically
* some people think of data as objects and want to organize them
* others think of the objects as part of a greater whole, and want to study
  the map/relation of inputs to outputs
* apportioned - inputs have semantic meaning

### Tasks

Levels of tasks:

1. descriptive statistics
2. think about data as a generating process from inputs to outputs

path to organizing data:

1. build a table
2. add a trivial hash which basically just treats each object different
3. make hash indicative of generating process
    * groups in outputs map to groups in inputs

* dimension reduction only has distances versus having the point information
* testing causality
    * re-evaluation is one method
    * how to cheaply validate models for causality?
* without testing properly we can find that storks deliver babies!

## Why we create models

* finance: do people build models to make predictions or to study a
  process? - almost entirely about prediction
* the generating process vs organizational statements

2 groups of people:

1. want to understand the underlying process
2. just want to understand their data

* are we back to Aristoltlean observational science?
    * Aristotle - just collected data
    * Bacon - wanted to connect inputs and outputs
    * recording and organizing data
    * maybe 10 years from now, people may study generative models
* once a theory is mathematically written down, now you can act on the model
* correlations are the stimuli for understanding the world, then you can
  start with trying to come up hypotheses with causalities

building a model:

1. make model predictive
2. if scientist: study model and find insight
2. if ML person: capitalize on predictions


## Meaning of "uncertainty"

* uncertainty in simulation: not all simulations have uncertainty. In
  simulation models there is no uncertainty with deterministic output.
* uncertainty with model is due to output being losely connected
  with the input
* if inverse mapping is ill-posed, then you'll get a collection of inputs
* alternative view: uncertainty in output is due to not knowing how the
  parameters are set. inputs have a probability distribution of how they
  might be set
* would variation be a better term?
* uncertainty may be a category, not a thing.  There are nuanced usages and
  broad, umbrella usages.

### Sources of uncertainty

* elatoric vs epistemic
* variation vs uncertainty

## Follow-ups

* reread ch 2 with the perspective of a ML person
